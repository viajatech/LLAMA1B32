#pip install langdetect transformers torch

import tkinter as tk
from tkinter import scrolledtext
from tkinter import ttk
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import threading
from langdetect import detect

# Token de acceso personal de Hugging Face
HF_TOKEN = "PON AQUI TU TOKEN READ!!!!"

# Configurar dispositivo (GPU si está disponible)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Nombre del modelo
model_name = "meta-llama/Llama-3.2-1B"

def load_model():
    status_label.config(text="Cargando el modelo, por favor espera...")
    progress_bar.start()

    def model_loading():
        try:
            # Cargar el tokenizador y el modelo
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                use_auth_token=HF_TOKEN,
                trust_remote_code=True
            )
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                use_auth_token=HF_TOKEN,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto",
                trust_remote_code=True
            )
            model.to(device)
            status_label.config(text="Modelo cargado. ¡Ya puedes chatear!")
            send_button.config(state=tk.NORMAL)
            user_entry.config(state=tk.NORMAL)
            root.model = model
            root.tokenizer = tokenizer
        except Exception as e:
            status_label.config(text=f"Error al cargar el modelo: {e}")
        finally:
            progress_bar.stop()

    threading.Thread(target=model_loading).start()

def generate_response():
    user_input = user_entry.get()
    user_entry.delete(0, tk.END)
    chat_log.config(state=tk.NORMAL)
    chat_log.insert(tk.END, f"{user_name.get()}: {user_input}\n")
    chat_log.config(state=tk.DISABLED)
    chat_log.see(tk.END)

    # Detectar el idioma del usuario
    try:
        language = detect(user_input)
    except:
        language = 'es'  # Asumimos español por defecto

    # Historial de la conversación
    if not hasattr(root, 'conversation_history'):
        root.conversation_history = ""

    # Crear un prompt específico según el idioma
    if language == 'es':
        prompt = (
            f"Eres un asistente de inteligencia artificial llamado {bot_name.get()} que responde en español. "
            "Debes seguir las instrucciones al pie de la letra, proporcionar respuestas lógicas y coherentes, "
            "y mantenerte en el tema de conversación.\n\n"
        )
    else:
        prompt = (
            f"You are an AI assistant named {bot_name.get()} who responds in English. "
            "You must follow the instructions precisely, provide logical and coherent responses, "
            "and stay on the conversation topic.\n\n"
        )

    root.conversation_history += f"{user_name.get()}: {user_input}\n"

    def run_model():
        try:
            with torch.no_grad():
                input_text = prompt + root.conversation_history
                input_ids = root.tokenizer.encode(input_text, return_tensors='pt').to(device)
                generated_ids = root.model.generate(
                    input_ids,
                    max_new_tokens=int(max_words.get()),  # Número máximo de tokens basado en la entrada
                    no_repeat_ngram_size=3,
                    do_sample=True,
                    top_k=50,
                    top_p=0.9,
                    temperature=0.6,
                    eos_token_id=root.tokenizer.eos_token_id,
                    pad_token_id=root.tokenizer.eos_token_id
                )
                response = root.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
                generated_text = response[len(input_text):].strip()
                # Asegurarse de que la respuesta esté en el idioma correcto
                root.conversation_history += f"{bot_name.get()}: {generated_text}\n"
                chat_log.config(state=tk.NORMAL)
                chat_log.insert(tk.END, f"{bot_name.get()}: {generated_text}\n\n")
                chat_log.config(state=tk.DISABLED)
                chat_log.see(tk.END)
        except Exception as e:
            chat_log.config(state=tk.NORMAL)
            chat_log.insert(tk.END, f"Error al generar la respuesta: {e}\n\n")
            chat_log.config(state=tk.DISABLED)
            chat_log.see(tk.END)

    threading.Thread(target=run_model).start()

# Configuración de la interfaz gráfica
root = tk.Tk()
root.title("Chat by Viaja Tech")
root.configure(bg='black')
root.geometry('800x700')

# Área de chat con scroll
chat_log = scrolledtext.ScrolledText(root, state=tk.DISABLED, bg='black', fg='white', wrap=tk.WORD)
chat_log.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)

# Frame para opciones adicionales
options_frame = tk.Frame(root, bg='black')
options_frame.pack(padx=10, pady=5, fill=tk.X)

# Campo para ingresar el nombre del usuario
tk.Label(options_frame, text="Tu nombre:", bg='black', fg='white').pack(side=tk.LEFT)
user_name = tk.Entry(options_frame, width=15, bg='gray20', fg='white')
user_name.insert(0, "Tú")  # Nombre por defecto
user_name.pack(side=tk.LEFT, padx=5)

# Campo para ingresar el nombre del bot
tk.Label(options_frame, text="Nombre del bot:", bg='black', fg='white').pack(side=tk.LEFT)
bot_name = tk.Entry(options_frame, width=15, bg='gray20', fg='white')
bot_name.insert(0, "Bot")  # Nombre por defecto
bot_name.pack(side=tk.LEFT, padx=5)

# Campo para seleccionar el número máximo de palabras en la respuesta
tk.Label(options_frame, text="Máx. palabras:", bg='black', fg='white').pack(side=tk.LEFT)
max_words = tk.Entry(options_frame, width=5, bg='gray20', fg='white')
max_words.insert(0, "150")  # Valor por defecto
max_words.pack(side=tk.LEFT, padx=5)

# Campo de entrada de usuario
user_entry = tk.Entry(root, width=100, bg='gray20', fg='white', state=tk.DISABLED)
user_entry.pack(padx=10, pady=(0, 10), fill=tk.X)

# Botón enviar
send_button = tk.Button(root, text="Enviar", command=generate_response, bg='gray30', fg='white', state=tk.DISABLED)
send_button.pack(pady=(0, 10))

# Etiqueta de estado y barra de progreso
status_label = tk.Label(root, text="Iniciando...", bg='black', fg='white')
status_label.pack(pady=(0, 5))

progress_bar = ttk.Progressbar(root, mode='indeterminate')
progress_bar.pack(padx=10, fill=tk.X)

# Mensaje inicial o historia
chat_log.config(state=tk.NORMAL)
chat_log.insert(tk.END, "Bienvenido al Chat de Viaja Tech. Antes de empezar, te contaré una historia...\n\n")
chat_log.config(state=tk.DISABLED)

# Iniciar carga del modelo
load_model()

# Iniciar la interfaz gráfica
root.mainloop()

