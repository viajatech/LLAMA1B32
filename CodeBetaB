#pip install transformers torch langdetect


import tkinter as tk
from tkinter import scrolledtext
from tkinter import ttk
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import threading
from langdetect import detect

# Token de acceso personal de Hugging Face
HF_TOKEN = "PON AQUI TU TOKEN READ!!!!"

# Configurar dispositivo (GPU si está disponible)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Nombre del modelo
model_name = "meta-llama/Llama-3.2-1B"

def load_model():
    status_label.config(text="Cargando el modelo, por favor espera...")
    progress_bar.start()

    def model_loading():
        try:
            # Cargar el tokenizador y el modelo
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                use_auth_token=HF_TOKEN,
                trust_remote_code=True
            )
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                use_auth_token=HF_TOKEN,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto",
                trust_remote_code=True
            )
            model.to(device)
            status_label.config(text="Modelo cargado. ¡Ya puedes chatear!")
            send_button.config(state=tk.NORMAL)
            user_entry.config(state=tk.NORMAL)
            clear_button.config(state=tk.NORMAL)
            root.model = model
            root.tokenizer = tokenizer
        except Exception as e:
            status_label.config(text=f"Error al cargar el modelo: {e}")
        finally:
            progress_bar.stop()

    threading.Thread(target=model_loading).start()

def generate_response(event=None):
    user_input = user_entry.get()
    if not user_input.strip():
        return  # No enviar si la entrada está vacía
    user_entry.delete(0, tk.END)
    chat_log.config(state=tk.NORMAL)
    chat_log.insert(tk.END, f"{user_name.get()}: {user_input}\n")
    chat_log.config(state=tk.DISABLED)
    chat_log.see(tk.END)

    # Detectar el idioma del usuario
    try:
        language = detect(user_input)
    except:
        language = 'es'  # Asumimos español por defecto

    # Historial de la conversación
    if not hasattr(root, 'conversation_history'):
        root.conversation_history = []

    # Crear un prompt específico según el idioma, incluyendo ejemplos y formato claro
    if language == 'es':
        prompt = (
            f"Eres un asistente de inteligencia artificial llamado {bot_name.get()} que responde en español. "
            f"Debes seguir las instrucciones al pie de la letra, proporcionar respuestas lógicas y coherentes, "
            f"y mantenerte en el tema de conversación. Si alguien te llama por tu nombre, {bot_name.get()}, debes responder adecuadamente.\n"
            f"Formato de la conversación:\n"
            "<USER>: Hola, ¿cómo estás?\n"
            f"<{bot_name.get()}>: Estoy bien, gracias. ¿En qué puedo ayudarte?\n\n"
            "Comencemos la conversación:\n"
        )
    else:
        prompt = (
            f"You are an AI assistant named {bot_name.get()} who responds in English. "
            f"You must follow the instructions precisely, provide logical and coherent responses, "
            f"and stay on the conversation topic. If someone calls you by your name, {bot_name.get()}, you should respond appropriately.\n"
            f"Conversation format:\n"
            "<USER>: Hi, how are you?\n"
            f"<{bot_name.get()}>: I'm good, thank you. How can I assist you today?\n\n"
            "Let's start the conversation:\n"
        )

    # Añadir la última interacción al historial con formato claro
    root.conversation_history.append(f"<{user_name.get()}>: {user_input}\n")

    # Limitar el historial a las últimas N líneas
    MAX_HISTORY_LINES = 10  # Puedes ajustar este valor
    if len(root.conversation_history) > MAX_HISTORY_LINES:
        root.conversation_history = root.conversation_history[-MAX_HISTORY_LINES:]

    def run_model():
        try:
            with torch.no_grad():
                input_text = prompt + ''.join(root.conversation_history) + f"<{bot_name.get()}>:"
                input_ids = root.tokenizer.encode(input_text, return_tensors='pt').to(device)
                generated_ids = root.model.generate(
                    input_ids,
                    max_new_tokens=int(max_words.get()),
                    no_repeat_ngram_size=3,
                    do_sample=True,
                    top_k=40,
                    top_p=0.8,
                    temperature=0.5,
                    eos_token_id=root.tokenizer.eos_token_id,
                    pad_token_id=root.tokenizer.eos_token_id
                )
                response = root.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
                # Extraer solo la respuesta del bot
                generated_text = response[len(input_text):].split(f"<{user_name.get()}>")[0].strip()

                # Añadir la respuesta al historial
                root.conversation_history.append(f"<{bot_name.get()}>: {generated_text}\n")

                # Mostrar la respuesta en la interfaz
                chat_log.config(state=tk.NORMAL)
                chat_log.insert(tk.END, f"{bot_name.get()}: {generated_text}\n\n")
                chat_log.config(state=tk.DISABLED)
                chat_log.see(tk.END)
        except Exception as e:
            chat_log.config(state=tk.NORMAL)
            chat_log.insert(tk.END, f"Error al generar la respuesta: {e}\n\n")
            chat_log.config(state=tk.DISABLED)
            chat_log.see(tk.END)

    threading.Thread(target=run_model).start()

def clear_chat(event=None):
    chat_log.config(state=tk.NORMAL)
    chat_log.delete(1.0, tk.END)
    chat_log.insert(tk.END, "Chat borrado. Puedes comenzar una nueva conversación.\n\n")
    chat_log.config(state=tk.DISABLED)
    # Reiniciar el historial de conversación
    if hasattr(root, 'conversation_history'):
        del root.conversation_history

# Configuración de la interfaz gráfica
root = tk.Tk()
root.title("Chat by Viaja Tech")
root.configure(bg='black')
root.geometry('800x700')

# Área de chat con scroll
chat_log = scrolledtext.ScrolledText(root, state=tk.DISABLED, bg='black', fg='white', wrap=tk.WORD)
chat_log.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)

# Frame para opciones adicionales
options_frame = tk.Frame(root, bg='black')
options_frame.pack(padx=10, pady=5, fill=tk.X)

# Campo para ingresar el nombre del usuario
tk.Label(options_frame, text="Tu nombre:", bg='black', fg='white').pack(side=tk.LEFT)
user_name = tk.Entry(options_frame, width=15, bg='gray20', fg='white')
user_name.insert(0, "Usuario")  # Nombre por defecto
user_name.pack(side=tk.LEFT, padx=5)

# Campo para ingresar el nombre del bot
tk.Label(options_frame, text="Nombre del bot:", bg='black', fg='white').pack(side=tk.LEFT)
bot_name = tk.Entry(options_frame, width=15, bg='gray20', fg='white')
bot_name.insert(0, "Asistente")  # Nombre por defecto
bot_name.pack(side=tk.LEFT, padx=5)

# Campo para seleccionar el número máximo de palabras en la respuesta
tk.Label(options_frame, text="Máx. palabras:", bg='black', fg='white').pack(side=tk.LEFT)
max_words = tk.Entry(options_frame, width=5, bg='gray20', fg='white')
max_words.insert(0, "150")  # Valor por defecto
max_words.pack(side=tk.LEFT, padx=5)

# Botón para borrar el chat
clear_button = tk.Button(options_frame, text="Borrar Chat", command=clear_chat, bg='gray30', fg='white', state=tk.DISABLED)
clear_button.pack(side=tk.RIGHT, padx=5)

# Campo de entrada de usuario
user_entry = tk.Entry(root, width=100, bg='gray20', fg='white', state=tk.DISABLED)
user_entry.pack(padx=10, pady=(0, 10), fill=tk.X)
user_entry.bind("<Return>", generate_response)  # Enviar mensaje con Enter
root.bind("<Delete>", clear_chat)  # Borrar chat con Delete

# Botón enviar
send_button = tk.Button(root, text="Enviar", command=generate_response, bg='gray30', fg='white', state=tk.DISABLED)
send_button.pack(pady=(0, 10))

# Etiqueta de estado y barra de progreso
status_label = tk.Label(root, text="Iniciando...", bg='black', fg='white')
status_label.pack(pady=(0, 5))

progress_bar = ttk.Progressbar(root, mode='indeterminate')
progress_bar.pack(padx=10, fill=tk.X)

# Mensaje inicial o historia
chat_log.config(state=tk.NORMAL)
chat_log.insert(tk.END, "Bienvenido al Chat de Viaja Tech. Antes de empezar, te contaré una historia...\n\n")
chat_log.config(state=tk.DISABLED)

# Iniciar carga del modelo
load_model()

# Iniciar la interfaz gráfica
root.mainloop()
