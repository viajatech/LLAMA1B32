#David Ruiz (@viajatech) 

#pip install transformers torch tkinter
#pip install --upgrade transformers torch
#pip install transformers torch tkinter huggingface_hub



#Recuerda editar y poner tu token de Hugging Face!! el token que debes copiar es el de READ, primero crealo!!!!


import tkinter as tk
from tkinter import scrolledtext
from tkinter import ttk
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import threading
import time
from pathlib import Path
import gc
from huggingface_hub import model_info, hf_hub_download

# Token de acceso personal de Hugging Face
HF_TOKEN = "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"

# Configurar dispositivo (GPU si está disponible)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Variables globales para el modelo y el tokenizador
model = None
tokenizer = None
model_name = "meta-llama/Llama-3.2-1B"

# Historial de conversación
conversation_history = ""

def download_model_files():
    # Función para descargar los archivos del modelo y mostrar progreso
    model_dir = Path(f"./{model_name.replace('/', '_')}")
    model_dir.mkdir(parents=True, exist_ok=True)

    # Verificar si el modelo ya está descargado
    if list(model_dir.glob("*")):
        log_text.insert(tk.END, "El modelo ya está descargado.\n")
        log_text.see(tk.END)
        return model_dir

    # Obtener los archivos del modelo desde Hugging Face
    repo_info = model_info(model_name, token=HF_TOKEN)

    total_size = sum(file.size or 0 for file in repo_info.siblings)
    total_size_mb = total_size / (1024 * 1024)
    start_time = time.time()

    log_text.insert(tk.END, f"Tamaño total de descarga: {total_size_mb:.2f} MB\n")
    log_text.see(tk.END)

    downloaded_size = 0
    previous_files_downloaded_size = 0

    # Descargar los archivos con progreso
    for file in repo_info.siblings:
        file_path = model_dir / file.rfilename

        if file.size is None:
            log_text.insert(tk.END, f"Omitiendo archivo sin tamaño definido: {file.rfilename}\n")
            log_text.see(tk.END)
            continue

        if file_path.exists() and file_path.stat().st_size == file.size:
            # Archivo ya descargado completamente
            downloaded_size += file.size
            previous_files_downloaded_size += file.size
            continue

        def progress_callback(current, total):
            nonlocal downloaded_size
            downloaded_size = previous_files_downloaded_size + current

            elapsed_time = time.time() - start_time
            speed = downloaded_size / elapsed_time  # Bytes por segundo
            remaining_size = total_size - downloaded_size
            eta = remaining_size / speed if speed > 0 else 0

            log_text.delete('1.0', tk.END)
            log_text.insert(tk.END, f"Descargando: {file.rfilename}\n")
            log_text.insert(tk.END, f"Progreso total: {downloaded_size / (1024*1024):.2f} MB de {total_size_mb:.2f} MB\n")
            log_text.insert(tk.END, f"Velocidad: {speed / (1024*1024):.2f} MB/s\n")
            log_text.insert(tk.END, f"Tiempo restante estimado: {eta:.2f} segundos\n")
            log_text.see(tk.END)

        # Descargar el archivo con seguimiento de progreso
        hf_hub_download(
            repo_id=model_name,
            filename=file.rfilename,
            local_dir=model_dir,
            local_dir_use_symlinks=False,  # Evita problemas de symlinks en Windows
            resume_download=True,
            force_download=False,
            progress_callback=progress_callback,
            token=HF_TOKEN,
        )

        # Después de descargar el archivo, actualizamos el tamaño descargado
        previous_files_downloaded_size += file.size or 0
        downloaded_size = previous_files_downloaded_size

    return model_dir

def load_model():
    global model, tokenizer

    log_text.insert(tk.END, "Descargando y cargando el modelo...\n")
    log_text.see(tk.END)
    model_dir = download_model_files()

    # Liberar memoria antes de cargar el modelo
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # Cargar el tokenizador y el modelo
    start_time = time.time()
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        use_auth_token=HF_TOKEN,
        trust_remote_code=True
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_dir,
        use_auth_token=HF_TOKEN,
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
        device_map="auto",
        trust_remote_code=True
    )
    model.to(device)
    end_time = time.time()
    elapsed_time = end_time - start_time

    log_text.insert(tk.END, f"Modelo cargado en {elapsed_time:.2f} segundos.\n")
    log_text.insert(tk.END, "Abriendo la interfaz de chat...\n")
    log_text.see(tk.END)

    # Después de cargar el modelo, iniciar la interfaz gráfica
    root.after(1000, open_chat_gui)

def open_chat_gui():
    root.destroy()
    chat_gui = tk.Tk()
    chat_gui.title("Chatbot by ViajaTech")
    chat_gui.configure(bg='black')
    chat_gui.geometry('800x600')

    chat_log = scrolledtext.ScrolledText(
        chat_gui, state=tk.DISABLED, bg='black', fg='white', wrap=tk.WORD
    )
    chat_log.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)

    user_entry = tk.Entry(chat_gui, width=100, bg='gray20', fg='white')
    user_entry.pack(padx=10, pady=(0,10), fill=tk.X)
    user_entry.focus()

    # Vincular la tecla Enter para enviar mensajes
    user_entry.bind('<Return>', lambda event: generate_response(user_entry, chat_log))

    send_button = tk.Button(
        chat_gui, text="Enviar",
        command=lambda: generate_response(user_entry, chat_log),
        bg='gray30', fg='white'
    )
    send_button.pack(pady=(0,10))

    chat_gui.mainloop()

def generate_response(user_entry, chat_log):
    global conversation_history
    user_input = user_entry.get()
    user_entry.delete(0, tk.END)

    chat_log.config(state=tk.NORMAL)
    chat_log.insert(tk.END, f"Tú: {user_input}\n")
    chat_log.config(state=tk.DISABLED)
    chat_log.see(tk.END)

    # Actualizar el historial de conversación
    conversation_history += f"User: {user_input}\n"

    # Limitar la longitud del historial si es necesario
    MAX_HISTORY_LENGTH = 2048  # Ajusta este valor según tus necesidades
    if len(conversation_history) > MAX_HISTORY_LENGTH:
        conversation_history = conversation_history[-MAX_HISTORY_LENGTH:]

    # Función para ejecutar en un hilo separado
    def run_model():
        global conversation_history
        with torch.no_grad():
            prompt = conversation_history + "Assistant:"
            inputs = tokenizer(prompt, return_tensors="pt").to(device)

            outputs = model.generate(
                **inputs,
                max_new_tokens=150,
                no_repeat_ngram_size=3,
                do_sample=True,
                top_k=50,
                top_p=0.95,
                temperature=0.8,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.eos_token_id
            )

            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            # Extraer la respuesta del asistente
            response_text = response[len(prompt):].split("User:")[0].strip()

            # Actualizar el historial de conversación
            conversation_history += f"Assistant: {response_text}\n"

            # Limitar la longitud del historial si es necesario
            if len(conversation_history) > MAX_HISTORY_LENGTH:
                conversation_history = conversation_history[-MAX_HISTORY_LENGTH:]

            chat_log.config(state=tk.NORMAL)
            chat_log.insert(tk.END, f"Bot: {response_text}\n\n")
            chat_log.config(state=tk.DISABLED)
            chat_log.see(tk.END)

    # Crear y empezar hilo para generar la respuesta
    threading.Thread(target=run_model).start()

# Ventana de LOGS
root = tk.Tk()
root.title("Cargando modelo - Chatbot by ViajaTech")
root.geometry('600x400')

log_text = tk.Text(root, state=tk.NORMAL)
log_text.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)

# Iniciar carga del modelo en un hilo separado
threading.Thread(target=load_model).start()

root.mainloop()
