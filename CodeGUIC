#David Ruiz (@viajatech) 

#pip install transformers torch tkinter
#pip install --upgrade transformers torch
#pip install transformers torch tkinter huggingface_hub



#Recuerda editar y poner tu token de Hugging Face!! el token que debes copiar es el de READ, primero crealo!!!!



import tkinter as tk
from tkinter import scrolledtext
from tkinter import ttk
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import threading

# Token de acceso personal de Hugging Face
HF_TOKEN = "PON AQUI TU TOKEN READ!!!!"

# Configurar dispositivo (GPU si está disponible)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Nombre del modelo
model_name = "meta-llama/Llama-3.2-1B"

def load_model():
    status_label.config(text="Cargando el modelo, por favor espera...")
    progress_bar.start()

    def model_loading():
        try:
            # Cargar el tokenizador y el modelo
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                use_auth_token=HF_TOKEN,
                trust_remote_code=True
            )
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                use_auth_token=HF_TOKEN,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto",
                trust_remote_code=True
            )
            model.to(device)
            status_label.config(text="Modelo cargado. ¡Ya puedes chatear!")
            send_button.config(state=tk.NORMAL)
            user_entry.config(state=tk.NORMAL)
            root.model = model
            root.tokenizer = tokenizer
        except Exception as e:
            status_label.config(text=f"Error al cargar el modelo: {e}")
        finally:
            progress_bar.stop()

    threading.Thread(target=model_loading).start()

def generate_response():
    user_input = user_entry.get()
    user_entry.delete(0, tk.END)
    chat_log.config(state=tk.NORMAL)
    chat_log.insert(tk.END, "Tú: " + user_input + "\n")
    chat_log.config(state=tk.DISABLED)
    chat_log.see(tk.END)

    # Historial de la conversación
    if not hasattr(root, 'conversation_history'):
        root.conversation_history = """Eres un asistente útil que sigue las instrucciones al pie de la letra y mantiene la coherencia en los temas de conversación. Responde de manera directa y concisa.

"""
    root.conversation_history += f"Tú: {user_input}\n"

    def run_model():
        try:
            with torch.no_grad():
                input_ids = root.tokenizer.encode(root.conversation_history, return_tensors='pt').to(device)
                generated_ids = root.model.generate(
                    input_ids,
                    max_new_tokens=150,
                    no_repeat_ngram_size=3,
                    do_sample=True,
                    top_k=50,
                    top_p=0.95,
                    temperature=0.7,
                    eos_token_id=root.tokenizer.eos_token_id,
                    pad_token_id=root.tokenizer.eos_token_id
                )
                response = root.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
                generated_text = response[len(root.conversation_history):].strip()
                root.conversation_history += f"Bot: {generated_text}\n"
                chat_log.config(state=tk.NORMAL)
                chat_log.insert(tk.END, "Bot: " + generated_text + "\n\n")
                chat_log.config(state=tk.DISABLED)
                chat_log.see(tk.END)
        except Exception as e:
            chat_log.config(state=tk.NORMAL)
            chat_log.insert(tk.END, f"Error al generar la respuesta: {e}\n\n")
            chat_log.config(state=tk.DISABLED)
            chat_log.see(tk.END)

    threading.Thread(target=run_model).start()

# Configuración de la interfaz gráfica
root = tk.Tk()
root.title("Chat by Viaja Tech")
root.configure(bg='black')
root.geometry('800x600')

# Área de chat con scroll
chat_log = scrolledtext.ScrolledText(root, state=tk.DISABLED, bg='black', fg='white', wrap=tk.WORD)
chat_log.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)

# Mensaje inicial o historia
chat_log.config(state=tk.NORMAL)
chat_log.insert(tk.END, "Bienvenido al Chat de Viaja Tech. Antes de empezar, te contaré una historia...\n\n")
chat_log.config(state=tk.DISABLED)

# Campo de entrada de usuario
user_entry = tk.Entry(root, width=100, bg='gray20', fg='white', state=tk.DISABLED)
user_entry.pack(padx=10, pady=(0, 10), fill=tk.X)

# Botón enviar
send_button = tk.Button(root, text="Enviar", command=generate_response, bg='gray30', fg='white', state=tk.DISABLED)
send_button.pack(pady=(0, 10))

# Etiqueta de estado y barra de progreso
status_label = tk.Label(root, text="Iniciando...", bg='black', fg='white')
status_label.pack(pady=(0, 5))

progress_bar = ttk.Progressbar(root, mode='indeterminate')
progress_bar.pack(padx=10, fill=tk.X)

# Iniciar carga del modelo
load_model()

# Iniciar la interfaz gráfica
root.mainloop()
